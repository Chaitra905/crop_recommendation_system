{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3b4399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T11:29:18.995830Z",
     "iopub.status.busy": "2025-10-12T11:29:18.995405Z",
     "iopub.status.idle": "2025-10-12T11:29:37.866546Z",
     "shell.execute_reply": "2025-10-12T11:29:37.865766Z"
    },
    "papermill": {
     "duration": 18.88059,
     "end_time": "2025-10-12T11:29:37.868165",
     "exception": false,
     "start_time": "2025-10-12T11:29:18.987575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications import (\n",
    "    ResNet50, ResNet101, ResNet152,\n",
    "    EfficientNetB0, EfficientNetB3, EfficientNetB7,\n",
    "    DenseNet121, DenseNet169, DenseNet201,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ece411",
   "metadata": {},
   "outputs": [],
   "source": [
    "    InceptionV3, InceptionResNetV2,\n",
    "    MobileNetV2, MobileNetV3Large,\n",
    "    VGG16, VGG19,\n",
    "    Xception, NASNetLarge\n",
    ")\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau,\n",
    "    TensorBoard, LearningRateScheduler, CSVLogger\n",
    ")\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    roc_auc_score, cohen_kappa_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Others\n",
    "from collections import Counter\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de9757e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T11:29:37.879915Z",
     "iopub.status.busy": "2025-10-12T11:29:37.878971Z",
     "iopub.status.idle": "2025-10-12T11:29:37.883504Z",
     "shell.execute_reply": "2025-10-12T11:29:37.882796Z"
    },
    "papermill": {
     "duration": 0.011685,
     "end_time": "2025-10-12T11:29:37.884953",
     "exception": false,
     "start_time": "2025-10-12T11:29:37.873268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f12f790",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T11:29:37.895648Z",
     "iopub.status.busy": "2025-10-12T11:29:37.895345Z",
     "iopub.status.idle": "2025-10-12T11:29:38.507158Z",
     "shell.execute_reply": "2025-10-12T11:29:38.506271Z"
    },
    "papermill": {
     "duration": 0.618586,
     "end_time": "2025-10-12T11:29:38.508443",
     "exception": false,
     "start_time": "2025-10-12T11:29:37.889857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# GPU Configuration\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        print(f\"‚úì GPU Available: {physical_devices}\")\n",
    "    except:\n",
    "        print(\"‚úì GPU Available but memory growth setting failed\")\n",
    "else:\n",
    "    print(\"‚ö† Running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c3fb15d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T11:29:38.520217Z",
     "iopub.status.busy": "2025-10-12T11:29:38.519906Z",
     "iopub.status.idle": "2025-10-12T11:29:38.547873Z",
     "shell.execute_reply": "2025-10-12T11:29:38.546864Z"
    },
    "papermill": {
     "duration": 0.03598,
     "end_time": "2025-10-12T11:29:38.549500",
     "exception": false,
     "start_time": "2025-10-12T11:29:38.513520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1. DATA LOADING AND EXPLORATION\n",
    "# ============================================================================\n",
    "\n",
    "class SoilDataLoader:\n",
    "    \"\"\"Advanced data loader with multiple format support\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.data_type = None\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = []\n",
    "        \n",
    "    def load_image_directory(self, img_size=(224, 224)):\n",
    "        \"\"\"Load images directly from class folders\"\"\"\n",
    "        self.class_names = sorted([d.name for d in self.data_path.iterdir() if d.is_dir()])\n",
    "        print(f\"‚úì Found {len(self.class_names)} classes: {self.class_names}\")\n",
    "        \n",
    "        for class_idx, class_name in enumerate(self.class_names):\n",
    "            class_path = self.data_path / class_name\n",
    "            image_files = list(class_path.glob('*.jpg')) + \\\n",
    "                         list(class_path.glob('*.png')) + \\\n",
    "                         list(class_path.glob('*.jpeg')) + \\\n",
    "                         list(class_path.glob('*.JPG'))\n",
    "            \n",
    "            print(f\"   Loading {class_name}: {len(image_files)} images...\")\n",
    "            \n",
    "            for img_path in image_files:\n",
    "                try:\n",
    "                    img = cv2.imread(str(img_path))\n",
    "                    if img is None:\n",
    "                        continue\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    img = cv2.resize(img, img_size)\n",
    "                    self.images.append(img)\n",
    "                    self.labels.append(class_idx)\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "        self.images = np.array(self.images)\n",
    "        self.labels = np.array(self.labels)\n",
    "        print(f\"‚úì Successfully loaded {len(self.images)} images\")\n",
    "        return self.images, self.labels, self.class_names\n",
    "\n",
    "\n",
    "class SoilEDA:\n",
    "    \"\"\"Comprehensive Exploratory Data Analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, images, labels, class_names):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.class_names = class_names\n",
    "        self.results = {}\n",
    "        \n",
    "    def analyze_all(self):\n",
    "        \"\"\"Run all EDA analyses\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"EXPLORATORY DATA ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        self.basic_statistics()\n",
    "        self.class_distribution()\n",
    "        self.image_properties()\n",
    "        self.color_analysis()\n",
    "        self.texture_analysis()\n",
    "        self.generate_report()\n",
    "        \n",
    "    def basic_statistics(self):\n",
    "        \"\"\"Basic dataset statistics\"\"\"\n",
    "        print(\"\\nüìä BASIC STATISTICS\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Total samples: {len(self.images)}\")\n",
    "        print(f\"Number of classes: {len(self.class_names)}\")\n",
    "        print(f\"Image shape: {self.images[0].shape}\")\n",
    "        print(f\"Data type: {self.images.dtype}\")\n",
    "        print(f\"Value range: [{self.images.min()}, {self.images.max()}]\")\n",
    "        \n",
    "        self.results['total_samples'] = len(self.images)\n",
    "        self.results['num_classes'] = len(self.class_names)\n",
    "        \n",
    "    def class_distribution(self):\n",
    "        \"\"\"Analyze class distribution and imbalance\"\"\"\n",
    "        print(\"\\nüìà CLASS DISTRIBUTION\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        class_counts = Counter(self.labels)\n",
    "        for idx, count in sorted(class_counts.items()):\n",
    "            print(f\"{self.class_names[idx]}: {count} ({count/len(self.labels)*100:.2f}%)\")\n",
    "        \n",
    "        max_count = max(class_counts.values())\n",
    "        min_count = min(class_counts.values())\n",
    "        imbalance_ratio = max_count / min_count\n",
    "        print(f\"\\nImbalance Ratio: {imbalance_ratio:.2f}\")\n",
    "        \n",
    "        self.results['class_counts'] = class_counts\n",
    "        self.results['imbalance_ratio'] = imbalance_ratio\n",
    "        \n",
    "        # Visualize\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.bar(range(len(class_counts)), [class_counts[i] for i in sorted(class_counts.keys())])\n",
    "        plt.xlabel('Class')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Class Distribution')\n",
    "        plt.xticks(range(len(self.class_names)), self.class_names, rotation=45, ha='right')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.pie([class_counts[i] for i in sorted(class_counts.keys())], \n",
    "                labels=self.class_names, autopct='%1.1f%%')\n",
    "        plt.title('Class Distribution (%)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('eda_class_distribution.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "    def image_properties(self):\n",
    "        \"\"\"Analyze image properties\"\"\"\n",
    "        print(\"\\nüñºÔ∏è  IMAGE PROPERTIES\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        brightness = np.mean(self.images, axis=(1, 2, 3))\n",
    "        print(f\"Brightness - Mean: {brightness.mean():.2f}, Std: {brightness.std():.2f}\")\n",
    "        \n",
    "        contrast = np.std(self.images, axis=(1, 2, 3))\n",
    "        print(f\"Contrast - Mean: {contrast.mean():.2f}, Std: {contrast.std():.2f}\")\n",
    "        \n",
    "        self.results['brightness'] = {'mean': brightness.mean(), 'std': brightness.std()}\n",
    "        self.results['contrast'] = {'mean': contrast.mean(), 'std': contrast.std()}\n",
    "        \n",
    "    def color_analysis(self):\n",
    "        \"\"\"Analyze color distributions\"\"\"\n",
    "        print(\"\\nüé® COLOR ANALYSIS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for i, channel in enumerate(['Red', 'Green', 'Blue']):\n",
    "            channel_mean = np.mean(self.images[:, :, :, i])\n",
    "            channel_std = np.std(self.images[:, :, :, i])\n",
    "            print(f\"{channel} - Mean: {channel_mean:.2f}, Std: {channel_std:.2f}\")\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "        colors = ['red', 'green', 'blue']\n",
    "        channels = ['Red', 'Green', 'Blue']\n",
    "        \n",
    "        for i, (ax, color, channel) in enumerate(zip(axes, colors, channels)):\n",
    "            ax.hist(self.images[:, :, :, i].flatten(), bins=100, color=color, alpha=0.7)\n",
    "            ax.set_title(f'{channel} Channel Distribution')\n",
    "            ax.set_xlabel('Pixel Value')\n",
    "            ax.set_ylabel('Frequency')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('eda_color_distribution.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "    def texture_analysis(self):\n",
    "        \"\"\"Analyze texture features using edge detection\"\"\"\n",
    "        print(\"\\nüîç TEXTURE ANALYSIS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        sample_size = min(100, len(self.images))\n",
    "        edge_intensities = []\n",
    "        \n",
    "        for i in range(sample_size):\n",
    "            gray = cv2.cvtColor(self.images[i], cv2.COLOR_RGB2GRAY)\n",
    "            edges = cv2.Canny(gray, 50, 150)\n",
    "            edge_intensity = np.sum(edges) / (edges.shape[0] * edges.shape[1])\n",
    "            edge_intensities.append(edge_intensity)\n",
    "        \n",
    "        print(f\"Edge Intensity - Mean: {np.mean(edge_intensities):.4f}, Std: {np.std(edge_intensities):.4f}\")\n",
    "        self.results['edge_intensity'] = {'mean': np.mean(edge_intensities), 'std': np.std(edge_intensities)}\n",
    "        \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate comprehensive EDA report\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"EDA SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        fig, axes = plt.subplots(len(self.class_names), 5, figsize=(15, 3*len(self.class_names)))\n",
    "        if len(self.class_names) == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for class_idx in range(len(self.class_names)):\n",
    "            class_images = self.images[self.labels == class_idx][:5]\n",
    "            for i in range(min(5, len(class_images))):\n",
    "                if len(class_images) > i:\n",
    "                    axes[class_idx, i].imshow(class_images[i])\n",
    "                axes[class_idx, i].axis('off')\n",
    "                if i == 0:\n",
    "                    axes[class_idx, i].set_title(f'{self.class_names[class_idx]}', fontsize=10)\n",
    "        \n",
    "        plt.suptitle('Sample Images by Class', fontsize=14, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('eda_sample_images.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"‚úì EDA complete. Visualizations saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68738eeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T11:29:38.561059Z",
     "iopub.status.busy": "2025-10-12T11:29:38.560196Z",
     "iopub.status.idle": "2025-10-12T11:29:38.568782Z",
     "shell.execute_reply": "2025-10-12T11:29:38.567865Z"
    },
    "papermill": {
     "duration": 0.015789,
     "end_time": "2025-10-12T11:29:38.570261",
     "exception": false,
     "start_time": "2025-10-12T11:29:38.554472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. ADVANCED DATA PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "class AdvancedPreprocessor:\n",
    "    \"\"\"State-of-the-art preprocessing pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, images, labels, img_size=(224, 224)):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.img_size = img_size\n",
    "        \n",
    "    def normalize(self, method='imagenet'):\n",
    "        \"\"\"Multiple normalization strategies\"\"\"\n",
    "        if method == 'standard':\n",
    "            self.images = (self.images - np.mean(self.images)) / (np.std(self.images) + 1e-7)\n",
    "        elif method == 'minmax':\n",
    "            self.images = (self.images - self.images.min()) / (self.images.max() - self.images.min() + 1e-7)\n",
    "        elif method == 'imagenet':\n",
    "            mean = np.array([0.485, 0.456, 0.406]) * 255\n",
    "            std = np.array([0.229, 0.224, 0.225]) * 255\n",
    "            self.images = (self.images - mean) / std\n",
    "        \n",
    "        print(f\"‚úì Applied {method} normalization\")\n",
    "        return self.images\n",
    "    \n",
    "    def handle_imbalance(self, strategy='class_weights'):\n",
    "        \"\"\"Handle class imbalance\"\"\"\n",
    "        if strategy == 'class_weights':\n",
    "            class_weights = compute_class_weight(\n",
    "                'balanced',\n",
    "                classes=np.unique(self.labels),\n",
    "                y=self.labels\n",
    "            )\n",
    "            class_weight_dict = dict(enumerate(class_weights))\n",
    "            print(f\"‚úì Computed class weights: {class_weight_dict}\")\n",
    "            return class_weight_dict\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5feeb94b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T11:29:38.581503Z",
     "iopub.status.busy": "2025-10-12T11:29:38.581157Z",
     "iopub.status.idle": "2025-10-12T11:29:38.600281Z",
     "shell.execute_reply": "2025-10-12T11:29:38.599439Z"
    },
    "papermill": {
     "duration": 0.026506,
     "end_time": "2025-10-12T11:29:38.601706",
     "exception": false,
     "start_time": "2025-10-12T11:29:38.575200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. STATE-OF-THE-ART MODEL ARCHITECTURES\n",
    "# ============================================================================\n",
    "\n",
    "class ModelFactory:\n",
    "    \"\"\"Factory for creating state-of-the-art DL models\"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def create_model(self, model_name, pretrained=True):\n",
    "        \"\"\"Create model by name\"\"\"\n",
    "        models_dict = {\n",
    "            'resnet50': self.resnet50,\n",
    "            'resnet101': self.resnet101,\n",
    "            'efficientnetb0': self.efficientnetb0,\n",
    "            'efficientnetb3': self.efficientnetb3,\n",
    "            'densenet121': self.densenet121,\n",
    "            'densenet169': self.densenet169,\n",
    "            'inceptionv3': self.inceptionv3,\n",
    "            'mobilenetv2': self.mobilenetv2,\n",
    "            'xception': self.xception,\n",
    "            'vgg16': self.vgg16,\n",
    "            'custom_cnn': self.custom_cnn,\n",
    "        }\n",
    "        \n",
    "        if model_name not in models_dict:\n",
    "            raise ValueError(f\"Model {model_name} not found\")\n",
    "        \n",
    "        return models_dict[model_name](pretrained)\n",
    "    \n",
    "    def _build_head(self, base_model, dropout=0.5):\n",
    "        \"\"\"Build classification head\"\"\"\n",
    "        x = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(512, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(256, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout/2)(x)\n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "        return models.Model(inputs=base_model.input, outputs=outputs)\n",
    "    \n",
    "    def resnet50(self, pretrained=True):\n",
    "        weights = 'imagenet' if pretrained else None\n",
    "        base = ResNet50(weights=weights, include_top=False, input_shape=self.input_shape)\n",
    "        return self._build_head(base)\n",
    "    \n",
    "    def resnet101(self, pretrained=True):\n",
    "        weights = 'imagenet' if pretrained else None\n",
    "        base = ResNet101(weights=weights, include_top=False, input_shape=self.input_shape)\n",
    "        return self._build_head(base)\n",
    "    \n",
    "    def efficientnetb0(self, pretrained=True):\n",
    "        weights = 'imagenet' if pretrained else None\n",
    "        base = EfficientNetB0(weights=weights, include_top=False, input_shape=self.input_shape)\n",
    "        return self._build_head(base)\n",
    "    \n",
    "    def efficientnetb3(self, pretrained=True):\n",
    "        weights = 'imagenet' if pretrained else None\n",
    "        base = EfficientNetB3(weights=weights, include_top=False, input_shape=self.input_shape)\n",
    "        return self._build_head(base)\n",
    "    \n",
    "    def densenet121(self, pretrained=True):\n",
    "        weights = 'imagenet' if pretrained else None\n",
    "        base = DenseNet121(weights=weights, include_top=False, input_shape=self.input_shape)\n",
    "        return self._build_head(base)\n",
    "    \n",
    "    def densenet169(self, pretrained=True):\n",
    "        weights = 'imagenet' if pretrained else None\n",
    "        base = DenseNet169(weights=weights, include_top=False, input_shape=self.input_shape)\n",
    "        return self._build_head(base)\n",
    "    \n",
    "    def inceptionv3(self, pretrained=True):\n",
    "        weights = 'imagenet' if pretrained else None\n",
    "        base = InceptionV3(weights=weights, include_top=False, input_shape=self.input_shape)\n",
    "        return self._build_head(base)\n",
    "    \n",
    "    def mobilenetv2(self, pretrained=True):\n",
    "        weights = 'imagenet' if pretrained else None\n",
    "        base = MobileNetV2(weights=weights, include_top=False, input_shape=self.input_shape)\n",
    "        return self._build_head(base)\n",
    "    \n",
    "    def xception(self, pretrained=True):\n",
    "        weights = 'imagenet' if pretrained else None\n",
    "        base = Xception(weights=weights, include_top=False, input_shape=self.input_shape)\n",
    "        return self._build_head(base)\n",
    "    \n",
    "    def vgg16(self, pretrained=True):\n",
    "        weights = 'imagenet' if pretrained else None\n",
    "        base = VGG16(weights=weights, include_top=False, input_shape=self.input_shape)\n",
    "        return self._build_head(base)\n",
    "    \n",
    "    def custom_cnn(self, pretrained=False):\n",
    "        \"\"\"Custom CNN with residual connections and attention\"\"\"\n",
    "        inputs = layers.Input(shape=self.input_shape)\n",
    "        \n",
    "        x = layers.Conv2D(64, 7, strides=2, padding='same')(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "        \n",
    "        for filters in [64, 128, 256, 512]:\n",
    "            shortcut = x\n",
    "            \n",
    "            x = layers.Conv2D(filters, 3, padding='same')(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            x = layers.Activation('relu')(x)\n",
    "            x = layers.Conv2D(filters, 3, padding='same')(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            \n",
    "            if shortcut.shape[-1] != filters:\n",
    "                shortcut = layers.Conv2D(filters, 1, padding='same')(shortcut)\n",
    "                shortcut = layers.BatchNormalization()(shortcut)\n",
    "            \n",
    "            x = layers.Add()([x, shortcut])\n",
    "            x = layers.Activation('relu')(x)\n",
    "            x = layers.MaxPooling2D(2)(x)\n",
    "            \n",
    "            # Squeeze-and-Excitation block\n",
    "            se = layers.GlobalAveragePooling2D()(x)\n",
    "            se = layers.Dense(filters // 16, activation='relu')(se)\n",
    "            se = layers.Dense(filters, activation='sigmoid')(se)\n",
    "            se = layers.Reshape((1, 1, filters))(se)\n",
    "            x = layers.Multiply()([x, se])\n",
    "        \n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dense(512, activation='relu')(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        x = layers.Dense(256, activation='relu')(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "        \n",
    "        return models.Model(inputs, outputs, name='Custom_ResNet_Attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9bf76f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T11:29:38.612913Z",
     "iopub.status.busy": "2025-10-12T11:29:38.612537Z",
     "iopub.status.idle": "2025-10-12T11:29:38.625556Z",
     "shell.execute_reply": "2025-10-12T11:29:38.624265Z"
    },
    "papermill": {
     "duration": 0.020882,
     "end_time": "2025-10-12T11:29:38.627528",
     "exception": false,
     "start_time": "2025-10-12T11:29:38.606646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. TRAINING FRAMEWORK - FIXED VERSION\n",
    "# ============================================================================\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"Advanced training framework with multiple strategies - FIXED\"\"\"\n",
    "    \n",
    "    def __init__(self, model, model_name, X_train, y_train, X_val, y_val, class_weights=None):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.class_weights = class_weights\n",
    "        self.history = None\n",
    "        \n",
    "    def compile_model(self, optimizer='adam', learning_rate=0.001):\n",
    "        \"\"\"Compile model with advanced optimizers - FIXED\"\"\"\n",
    "        if optimizer == 'adam':\n",
    "            opt = optimizers.Adam(learning_rate=learning_rate)\n",
    "        elif optimizer == 'adamw':\n",
    "            opt = optimizers.AdamW(learning_rate=learning_rate)\n",
    "        elif optimizer == 'sgd':\n",
    "            opt = optimizers.SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)\n",
    "        elif optimizer == 'rmsprop':\n",
    "            opt = optimizers.RMSprop(learning_rate=learning_rate)\n",
    "        else:\n",
    "            opt = optimizers.Adam(learning_rate=learning_rate)\n",
    "        \n",
    "        # FIX: Use SparseTopKCategoricalAccuracy for sparse labels\n",
    "        self.model.compile(\n",
    "            optimizer=opt,\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy', \n",
    "                    tf.keras.metrics.SparseTopKCategoricalAccuracy(k=3, name='top3_accuracy')]\n",
    "        )\n",
    "    \n",
    "    def get_callbacks(self, patience=15):\n",
    "        \"\"\"Advanced callbacks\"\"\"\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=patience,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-7,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ModelCheckpoint(\n",
    "                f'best_{self.model_name}.h5',\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            CSVLogger(f'training_log_{self.model_name}.csv')\n",
    "        ]\n",
    "        return callbacks\n",
    "    \n",
    "    def train(self, epochs=100, batch_size=32, patience=15):\n",
    "        \"\"\"Train model\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Training {self.model_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        self.history = self.model.fit(\n",
    "            self.X_train, self.y_train,\n",
    "            validation_data=(self.X_val, self.y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            class_weight=self.class_weights,\n",
    "            callbacks=self.get_callbacks(patience=patience),\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Comprehensive evaluation\"\"\"\n",
    "        y_pred_probs = self.model.predict(self.X_val, verbose=0)\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "        \n",
    "        results = {\n",
    "            'accuracy': accuracy_score(self.y_val, y_pred),\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_pred_probs,\n",
    "            'history': self.history.history if self.history else None\n",
    "        }\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4934a2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T11:29:38.642633Z",
     "iopub.status.busy": "2025-10-12T11:29:38.642333Z",
     "iopub.status.idle": "2025-10-12T11:29:38.649336Z",
     "shell.execute_reply": "2025-10-12T11:29:38.648385Z"
    },
    "papermill": {
     "duration": 0.014319,
     "end_time": "2025-10-12T11:29:38.650718",
     "exception": false,
     "start_time": "2025-10-12T11:29:38.636399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. ENSEMBLE METHODS\n",
    "# ============================================================================\n",
    "\n",
    "class EnsembleModel:\n",
    "    \"\"\"Advanced ensemble methods\"\"\"\n",
    "    \n",
    "    def __init__(self, models, model_names):\n",
    "        self.models = models\n",
    "        self.model_names = model_names\n",
    "        \n",
    "    def predict_voting(self, X, method='soft'):\n",
    "        \"\"\"Voting ensemble\"\"\"\n",
    "        predictions = []\n",
    "        for model in self.models:\n",
    "            if method == 'soft':\n",
    "                predictions.append(model.predict(X, verbose=0))\n",
    "            else:\n",
    "                predictions.append(np.argmax(model.predict(X, verbose=0), axis=1))\n",
    "        \n",
    "        if method == 'soft':\n",
    "            avg_predictions = np.mean(predictions, axis=0)\n",
    "            return np.argmax(avg_predictions, axis=1), avg_predictions\n",
    "        else:\n",
    "            votes = np.array(predictions).T\n",
    "            return np.apply_along_axis(lambda x: np.bincount(x).argmax(), 1, votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "993b7723",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T11:29:38.661927Z",
     "iopub.status.busy": "2025-10-12T11:29:38.661595Z",
     "iopub.status.idle": "2025-10-12T11:29:38.681351Z",
     "shell.execute_reply": "2025-10-12T11:29:38.680520Z"
    },
    "papermill": {
     "duration": 0.027331,
     "end_time": "2025-10-12T11:29:38.682892",
     "exception": false,
     "start_time": "2025-10-12T11:29:38.655561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. COMPREHENSIVE EVALUATION AND VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"Comprehensive model evaluation and comparison\"\"\"\n",
    "    \n",
    "    def __init__(self, class_names):\n",
    "        self.class_names = class_names\n",
    "        self.results = {}\n",
    "        \n",
    "    def evaluate_model(self, model_name, y_true, y_pred, y_pred_probs, history=None):\n",
    "        \"\"\"Evaluate single model\"\"\"\n",
    "        results = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)[0],\n",
    "            'recall': precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)[1],\n",
    "            'f1_score': precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)[2],\n",
    "            'cohen_kappa': cohen_kappa_score(y_true, y_pred),\n",
    "            'matthews_corrcoef': matthews_corrcoef(y_true, y_pred),\n",
    "            'confusion_matrix': confusion_matrix(y_true, y_pred),\n",
    "            'classification_report': classification_report(y_true, y_pred, target_names=self.class_names, zero_division=0),\n",
    "            'history': history\n",
    "        }\n",
    "        \n",
    "        # ROC AUC for multi-class\n",
    "        try:\n",
    "            from sklearn.preprocessing import label_binarize\n",
    "            y_true_bin = label_binarize(y_true, classes=range(len(self.class_names)))\n",
    "            results['roc_auc'] = roc_auc_score(y_true_bin, y_pred_probs, average='weighted', multi_class='ovr')\n",
    "        except:\n",
    "            results['roc_auc'] = None\n",
    "        \n",
    "        self.results[model_name] = results\n",
    "        return results\n",
    "    \n",
    "    def print_results(self, model_name):\n",
    "        \"\"\"Print detailed results\"\"\"\n",
    "        results = self.results[model_name]\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"EVALUATION RESULTS: {model_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {results['precision']:.4f}\")\n",
    "        print(f\"Recall: {results['recall']:.4f}\")\n",
    "        print(f\"F1-Score: {results['f1_score']:.4f}\")\n",
    "        print(f\"Cohen's Kappa: {results['cohen_kappa']:.4f}\")\n",
    "        print(f\"Matthews Correlation: {results['matthews_corrcoef']:.4f}\")\n",
    "        if results['roc_auc']:\n",
    "            print(f\"ROC AUC: {results['roc_auc']:.4f}\")\n",
    "        print(f\"\\n{results['classification_report']}\")\n",
    "    \n",
    "    def plot_confusion_matrix(self, model_name):\n",
    "        \"\"\"Plot confusion matrix\"\"\"\n",
    "        cm = self.results[model_name]['confusion_matrix']\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=self.class_names, yticklabels=self.class_names)\n",
    "        plt.title(f'Confusion Matrix - {model_name}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'confusion_matrix_{model_name}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_training_history(self, model_name):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        history = self.results[model_name]['history']\n",
    "        if history is None:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Accuracy\n",
    "        axes[0].plot(history['accuracy'], label='Train', linewidth=2)\n",
    "        axes[0].plot(history['val_accuracy'], label='Validation', linewidth=2)\n",
    "        axes[0].set_title(f'Model Accuracy - {model_name}', fontsize=14)\n",
    "        axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "        axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "        axes[0].legend(fontsize=11)\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Loss\n",
    "        axes[1].plot(history['loss'], label='Train', linewidth=2)\n",
    "        axes[1].plot(history['val_loss'], label='Validation', linewidth=2)\n",
    "        axes[1].set_title(f'Model Loss - {model_name}', fontsize=14)\n",
    "        axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "        axes[1].set_ylabel('Loss', fontsize=12)\n",
    "        axes[1].legend(fontsize=11)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'training_history_{model_name}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def compare_models(self):\n",
    "        \"\"\"Compare all models\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"‚ö† No models to compare\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"MODEL COMPARISON\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        comparison_df = pd.DataFrame({\n",
    "            'Model': list(self.results.keys()),\n",
    "            'Accuracy': [r['accuracy'] for r in self.results.values()],\n",
    "            'Precision': [r['precision'] for r in self.results.values()],\n",
    "            'Recall': [r['recall'] for r in self.results.values()],\n",
    "            'F1-Score': [r['f1_score'] for r in self.results.values()],\n",
    "            'Cohen Kappa': [r['cohen_kappa'] for r in self.results.values()],\n",
    "            'Matthews Corr': [r['matthews_corrcoef'] for r in self.results.values()],\n",
    "        })\n",
    "        \n",
    "        comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
    "        print(comparison_df.to_string(index=False))\n",
    "        \n",
    "        comparison_df.to_csv('model_comparison.csv', index=False)\n",
    "        \n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "        metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Cohen Kappa', 'Matthews Corr']\n",
    "        \n",
    "        for idx, (ax, metric) in enumerate(zip(axes.flat, metrics)):\n",
    "            data = comparison_df.sort_values(metric, ascending=True)\n",
    "            ax.barh(data['Model'], data[metric])\n",
    "            ax.set_xlabel(metric, fontsize=11)\n",
    "            ax.set_title(f'{metric} Comparison', fontsize=12)\n",
    "            ax.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        return comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66c82993",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T11:29:38.694160Z",
     "iopub.status.busy": "2025-10-12T11:29:38.693331Z",
     "iopub.status.idle": "2025-10-12T11:29:38.700450Z",
     "shell.execute_reply": "2025-10-12T11:29:38.699529Z"
    },
    "papermill": {
     "duration": 0.014193,
     "end_time": "2025-10-12T11:29:38.701890",
     "exception": false,
     "start_time": "2025-10-12T11:29:38.687697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE SOIL CLASSIFICATION PIPELINE - FIXED VERSION\n",
      "================================================================================\n",
      "\n",
      "üìã Configuration:\n",
      "   Data Path: /kaggle/input/comprehensive-soil-classification-datasets/CyAUG-Dataset\n",
      "   Models: fast\n",
      "   Image Size: (224, 224)\n",
      "   Batch Size: 32\n",
      "   Epochs: 50\n",
      "   Patience: 10\n"
     ]
    }
   ],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "DATA_PATH = \"/kaggle/input/comprehensive-soil-classification-datasets/CyAUG-Dataset\"\n",
    "\n",
    "# Choose: 'all', 'fast', or custom list\n",
    "MODELS_TO_TRAIN = 'fast'  # Fast models for quick testing\n",
    "\n",
    "CONFIG = {\n",
    "    'img_size': (224, 224),\n",
    "    'batch_size': 32,\n",
    "    'epochs': 50,\n",
    "    'patience': 10,  # Early stopping patience\n",
    "    'learning_rate': 0.001,\n",
    "    'validation_split': 0.2,\n",
    "    'test_split': 0.2,\n",
    "    'optimizer': 'adam',\n",
    "    'normalization': 'imagenet',\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE SOIL CLASSIFICATION PIPELINE - FIXED VERSION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìã Configuration:\")\n",
    "print(f\"   Data Path: {DATA_PATH}\")\n",
    "print(f\"   Models: {MODELS_TO_TRAIN}\")\n",
    "print(f\"   Image Size: {CONFIG['img_size']}\")\n",
    "print(f\"   Batch Size: {CONFIG['batch_size']}\")\n",
    "print(f\"   Epochs: {CONFIG['epochs']}\")\n",
    "print(f\"   Patience: {CONFIG['patience']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14deb989",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T11:29:38.713872Z",
     "iopub.status.busy": "2025-10-12T11:29:38.713403Z",
     "iopub.status.idle": "2025-10-12T11:29:38.837636Z",
     "shell.execute_reply": "2025-10-12T11:29:38.836507Z"
    },
    "papermill": {
     "duration": 0.132695,
     "end_time": "2025-10-12T11:29:38.839827",
     "exception": false,
     "start_time": "2025-10-12T11:29:38.707132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Found 7 soil types:\n",
      "   - Alluvial_Soil: 49 images\n",
      "   - Arid_Soil: 284 images\n",
      "   - Black_Soil: 255 images\n",
      "   - Laterite_Soil: 219 images\n",
      "   - Mountain_Soil: 201 images\n",
      "   - Red_Soil: 108 images\n",
      "   - Yellow_Soil: 69 images\n"
     ]
    }
   ],
   "source": [
    "# ========== VERIFY DATA PATH ==========\n",
    "data_path = Path(DATA_PATH)\n",
    "\n",
    "if not data_path.exists():\n",
    "    print(f\"\\n‚ùå ERROR: Data path not found: {DATA_PATH}\")\n",
    "    print(\"Please update DATA_PATH variable\")\n",
    "    exit(1)\n",
    "\n",
    "soil_types = [d.name for d in data_path.iterdir() if d.is_dir()]\n",
    "print(f\"\\n‚úì Found {len(soil_types)} soil types:\")\n",
    "for soil_type in sorted(soil_types)[:10]:  # Show first 10\n",
    "    num_images = len(list((data_path / soil_type).glob('*.jpg'))) + \\\n",
    "                    len(list((data_path / soil_type).glob('*.png')))\n",
    "    print(f\"   - {soil_type}: {num_images} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c38dd89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T11:29:38.853482Z",
     "iopub.status.busy": "2025-10-12T11:29:38.852028Z",
     "iopub.status.idle": "2025-10-12T11:29:38.859904Z",
     "shell.execute_reply": "2025-10-12T11:29:38.858835Z"
    },
    "papermill": {
     "duration": 0.016056,
     "end_time": "2025-10-12T11:29:38.861985",
     "exception": false,
     "start_time": "2025-10-12T11:29:38.845929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Output directory: /kaggle/working/soil_classification_results/soil_classification_results\n"
     ]
    }
   ],
   "source": [
    "# ========== CREATE OUTPUT DIRECTORY ==========\n",
    "output_dir = Path('soil_classification_results')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "os.chdir(output_dir)\n",
    "print(f\"\\n‚úì Output directory: {output_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3fcff48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T11:29:38.878560Z",
     "iopub.status.busy": "2025-10-12T11:29:38.877812Z",
     "iopub.status.idle": "2025-10-12T11:30:28.683423Z",
     "shell.execute_reply": "2025-10-12T11:30:28.682528Z"
    },
    "papermill": {
     "duration": 49.813924,
     "end_time": "2025-10-12T11:30:28.684994",
     "exception": false,
     "start_time": "2025-10-12T11:29:38.871070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: DATA LOADING\n",
      "================================================================================\n",
      "‚úì Found 7 classes: ['Alluvial_Soil', 'Arid_Soil', 'Black_Soil', 'Laterite_Soil', 'Mountain_Soil', 'Red_Soil', 'Yellow_Soil']\n",
      "   Loading Alluvial_Soil: 692 images...\n",
      "   Loading Arid_Soil: 284 images...\n",
      "   Loading Black_Soil: 1173 images...\n",
      "   Loading Laterite_Soil: 219 images...\n",
      "   Loading Mountain_Soil: 201 images...\n",
      "   Loading Red_Soil: 1126 images...\n",
      "   Loading Yellow_Soil: 1401 images...\n",
      "‚úì Successfully loaded 5096 images\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load Data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: DATA LOADING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "loader = SoilDataLoader(data_path)\n",
    "images, labels, class_names = loader.load_image_directory(img_size=CONFIG['img_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28f370aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T11:30:28.697692Z",
     "iopub.status.busy": "2025-10-12T11:30:28.697034Z",
     "iopub.status.idle": "2025-10-12T11:31:01.223222Z",
     "shell.execute_reply": "2025-10-12T11:31:01.222220Z"
    },
    "papermill": {
     "duration": 32.539901,
     "end_time": "2025-10-12T11:31:01.230288",
     "exception": false,
     "start_time": "2025-10-12T11:30:28.690387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: EXPLORATORY DATA ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "======================================================================\n",
      "EXPLORATORY DATA ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "üìä BASIC STATISTICS\n",
      "--------------------------------------------------\n",
      "Total samples: 5096\n",
      "Number of classes: 7\n",
      "Image shape: (224, 224, 3)\n",
      "Data type: uint8\n",
      "Value range: [0, 255]\n",
      "\n",
      "üìà CLASS DISTRIBUTION\n",
      "--------------------------------------------------\n",
      "Alluvial_Soil: 692 (13.58%)\n",
      "Arid_Soil: 284 (5.57%)\n",
      "Black_Soil: 1173 (23.02%)\n",
      "Laterite_Soil: 219 (4.30%)\n",
      "Mountain_Soil: 201 (3.94%)\n",
      "Red_Soil: 1126 (22.10%)\n",
      "Yellow_Soil: 1401 (27.49%)\n",
      "\n",
      "Imbalance Ratio: 6.97\n",
      "\n",
      "üñºÔ∏è  IMAGE PROPERTIES\n",
      "--------------------------------------------------\n",
      "Brightness - Mean: 104.19, Std: 41.28\n",
      "Contrast - Mean: 54.17, Std: 19.18\n",
      "\n",
      "üé® COLOR ANALYSIS\n",
      "--------------------------------------------------\n",
      "Red - Mean: 149.03, Std: 70.33\n",
      "Green - Mean: 100.95, Std: 59.91\n",
      "Blue - Mean: 62.59, Std: 52.28\n",
      "\n",
      "üîç TEXTURE ANALYSIS\n",
      "--------------------------------------------------\n",
      "Edge Intensity - Mean: 80.6087, Std: 17.9521\n",
      "\n",
      "======================================================================\n",
      "EDA SUMMARY\n",
      "======================================================================\n",
      "‚úì EDA complete. Visualizations saved.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: EDA\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "eda = SoilEDA(images, labels, class_names)\n",
    "eda.analyze_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8758c015",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T11:31:01.243719Z",
     "iopub.status.busy": "2025-10-12T11:31:01.243000Z",
     "iopub.status.idle": "2025-10-12T11:31:13.844829Z",
     "shell.execute_reply": "2025-10-12T11:31:13.843787Z"
    },
    "papermill": {
     "duration": 12.610191,
     "end_time": "2025-10-12T11:31:13.846409",
     "exception": false,
     "start_time": "2025-10-12T11:31:01.236218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: DATA PREPROCESSING\n",
      "================================================================================\n",
      "‚úì Applied imagenet normalization\n",
      "‚úì Computed class weights: {0: 1.0520231213872833, 1: 2.563380281690141, 2: 0.6206308610400681, 3: 3.324200913242009, 4: 3.6218905472636815, 5: 0.6465364120781527, 6: 0.5196288365453248}\n",
      "\n",
      "‚úì Dataset Split:\n",
      "   Train: 3260 (64.0%)\n",
      "   Validation: 816 (16.0%)\n",
      "   Test: 1020 (20.0%)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Preprocessing\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: DATA PREPROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "preprocessor = AdvancedPreprocessor(images, labels)\n",
    "images = preprocessor.normalize(method=CONFIG['normalization'])\n",
    "class_weights = preprocessor.handle_imbalance()\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    images, labels, \n",
    "    test_size=CONFIG['test_split'], \n",
    "    stratify=labels, \n",
    "    random_state=42\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, \n",
    "    test_size=CONFIG['validation_split'], \n",
    "    stratify=y_train, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Dataset Split:\")\n",
    "print(f\"   Train: {len(X_train)} ({len(X_train)/len(images)*100:.1f}%)\")\n",
    "print(f\"   Validation: {len(X_val)} ({len(X_val)/len(images)*100:.1f}%)\")\n",
    "print(f\"   Test: {len(X_test)} ({len(X_test)/len(images)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84d66920",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T11:31:13.861956Z",
     "iopub.status.busy": "2025-10-12T11:31:13.861519Z",
     "iopub.status.idle": "2025-10-12T11:58:50.430979Z",
     "shell.execute_reply": "2025-10-12T11:58:50.430059Z"
    },
    "papermill": {
     "duration": 1656.579383,
     "end_time": "2025-10-12T11:58:50.432276",
     "exception": false,
     "start_time": "2025-10-12T11:31:13.852893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 4: MODEL TRAINING\n",
      "================================================================================\n",
      "\n",
      "üìã Models to train: mobilenetv2, efficientnetb0, resnet50\n",
      "\n",
      "================================================================================\n",
      "[1/3] TRAINING: MOBILENETV2\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760268673.994263      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "‚úì Model created: 3,054,151 parameters\n",
      "\n",
      "======================================================================\n",
      "Training mobilenetv2\n",
      "======================================================================\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1760268731.091696      62 service.cc:148] XLA service 0x78209c0036d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1760268731.092564      62 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1760268735.540924      62 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "E0000 00:00:1760268742.917865      62 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1760268743.114765      62 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/102\u001b[0m \u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m2:08:51\u001b[0m 77s/step - accuracy: 0.0938 - loss: 2.2886 - top3_accuracy: 0.4375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760268759.347838      62 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.5760 - loss: 1.3208 - top3_accuracy: 0.8015"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1760268778.310948      62 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1760268778.509986      62 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355ms/step - accuracy: 0.5775 - loss: 1.3179 - top3_accuracy: 0.8024\n",
      "Epoch 1: val_accuracy improved from -inf to 0.40931, saving model to best_mobilenetv2.h5\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 477ms/step - accuracy: 0.5789 - loss: 1.3150 - top3_accuracy: 0.8034 - val_accuracy: 0.4093 - val_loss: 5.7818 - val_top3_accuracy: 0.8015 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8384 - loss: 0.6397 - top3_accuracy: 0.9562\n",
      "Epoch 2: val_accuracy improved from 0.40931 to 0.61765, saving model to best_mobilenetv2.h5\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 96ms/step - accuracy: 0.8384 - loss: 0.6395 - top3_accuracy: 0.9562 - val_accuracy: 0.6176 - val_loss: 8.7473 - val_top3_accuracy: 0.7255 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8785 - loss: 0.4546 - top3_accuracy: 0.9703\n",
      "Epoch 3: val_accuracy did not improve from 0.61765\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 91ms/step - accuracy: 0.8786 - loss: 0.4544 - top3_accuracy: 0.9703 - val_accuracy: 0.5784 - val_loss: 4.5757 - val_top3_accuracy: 0.8370 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8948 - loss: 0.3742 - top3_accuracy: 0.9736\n",
      "Epoch 4: val_accuracy did not improve from 0.61765\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 89ms/step - accuracy: 0.8947 - loss: 0.3747 - top3_accuracy: 0.9736 - val_accuracy: 0.5466 - val_loss: 5.0754 - val_top3_accuracy: 0.7880 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8902 - loss: 0.4019 - top3_accuracy: 0.9733\n",
      "Epoch 5: val_accuracy did not improve from 0.61765\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 90ms/step - accuracy: 0.8904 - loss: 0.4015 - top3_accuracy: 0.9734 - val_accuracy: 0.6017 - val_loss: 7.0115 - val_top3_accuracy: 0.8517 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9278 - loss: 0.2506 - top3_accuracy: 0.9871\n",
      "Epoch 6: val_accuracy improved from 0.61765 to 0.64583, saving model to best_mobilenetv2.h5\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 96ms/step - accuracy: 0.9279 - loss: 0.2506 - top3_accuracy: 0.9871 - val_accuracy: 0.6458 - val_loss: 5.7100 - val_top3_accuracy: 0.8787 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9380 - loss: 0.2267 - top3_accuracy: 0.9924\n",
      "Epoch 7: val_accuracy did not improve from 0.64583\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 90ms/step - accuracy: 0.9380 - loss: 0.2273 - top3_accuracy: 0.9924 - val_accuracy: 0.4779 - val_loss: 6.1777 - val_top3_accuracy: 0.7426 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9291 - loss: 0.2752 - top3_accuracy: 0.9863\n",
      "Epoch 8: val_accuracy improved from 0.64583 to 0.76471, saving model to best_mobilenetv2.h5\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 97ms/step - accuracy: 0.9293 - loss: 0.2747 - top3_accuracy: 0.9864 - val_accuracy: 0.7647 - val_loss: 2.3119 - val_top3_accuracy: 0.9020 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9633 - loss: 0.1458 - top3_accuracy: 0.9959\n",
      "Epoch 9: val_accuracy did not improve from 0.76471\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 90ms/step - accuracy: 0.9632 - loss: 0.1460 - top3_accuracy: 0.9959 - val_accuracy: 0.6495 - val_loss: 7.5175 - val_top3_accuracy: 0.9289 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9465 - loss: 0.2137 - top3_accuracy: 0.9910\n",
      "Epoch 10: val_accuracy did not improve from 0.76471\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 90ms/step - accuracy: 0.9465 - loss: 0.2139 - top3_accuracy: 0.9910 - val_accuracy: 0.5674 - val_loss: 13.4579 - val_top3_accuracy: 0.8358 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9326 - loss: 0.2437 - top3_accuracy: 0.9903\n",
      "Epoch 11: val_accuracy did not improve from 0.76471\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 89ms/step - accuracy: 0.9327 - loss: 0.2435 - top3_accuracy: 0.9903 - val_accuracy: 0.2610 - val_loss: 13.2990 - val_top3_accuracy: 0.4951 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9547 - loss: 0.1631 - top3_accuracy: 0.9913\n",
      "Epoch 12: val_accuracy did not improve from 0.76471\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 89ms/step - accuracy: 0.9547 - loss: 0.1630 - top3_accuracy: 0.9913 - val_accuracy: 0.3100 - val_loss: 14.4814 - val_top3_accuracy: 0.4412 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9483 - loss: 0.1555 - top3_accuracy: 0.9892\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.76471\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 88ms/step - accuracy: 0.9484 - loss: 0.1556 - top3_accuracy: 0.9892 - val_accuracy: 0.3885 - val_loss: 13.1227 - val_top3_accuracy: 0.6593 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9611 - loss: 0.1370 - top3_accuracy: 0.9958\n",
      "Epoch 14: val_accuracy did not improve from 0.76471\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 89ms/step - accuracy: 0.9612 - loss: 0.1366 - top3_accuracy: 0.9958 - val_accuracy: 0.4681 - val_loss: 12.4438 - val_top3_accuracy: 0.7659 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9841 - loss: 0.0658 - top3_accuracy: 0.9991\n",
      "Epoch 15: val_accuracy did not improve from 0.76471\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 89ms/step - accuracy: 0.9842 - loss: 0.0656 - top3_accuracy: 0.9991 - val_accuracy: 0.4755 - val_loss: 10.5818 - val_top3_accuracy: 0.6900 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9931 - loss: 0.0231 - top3_accuracy: 1.0000\n",
      "Epoch 16: val_accuracy did not improve from 0.76471\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 88ms/step - accuracy: 0.9931 - loss: 0.0231 - top3_accuracy: 1.0000 - val_accuracy: 0.5049 - val_loss: 7.7087 - val_top3_accuracy: 0.8346 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9939 - loss: 0.0173 - top3_accuracy: 0.9984\n",
      "Epoch 17: val_accuracy did not improve from 0.76471\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 89ms/step - accuracy: 0.9940 - loss: 0.0172 - top3_accuracy: 0.9984 - val_accuracy: 0.5551 - val_loss: 5.6075 - val_top3_accuracy: 0.8591 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9967 - loss: 0.0098 - top3_accuracy: 1.0000\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.76471\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 89ms/step - accuracy: 0.9968 - loss: 0.0098 - top3_accuracy: 1.0000 - val_accuracy: 0.6176 - val_loss: 3.8983 - val_top3_accuracy: 0.8934 - learning_rate: 5.0000e-04\n",
      "Epoch 18: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\n",
      "======================================================================\n",
      "EVALUATION RESULTS: mobilenetv2\n",
      "======================================================================\n",
      "Accuracy: 0.7647\n",
      "Precision: 0.8512\n",
      "Recall: 0.7647\n",
      "F1-Score: 0.7738\n",
      "Cohen's Kappa: 0.7107\n",
      "Matthews Correlation: 0.7300\n",
      "ROC AUC: 0.9659\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Alluvial_Soil       0.93      0.33      0.49       111\n",
      "    Arid_Soil       0.53      0.35      0.42        46\n",
      "   Black_Soil       0.86      0.97      0.91       188\n",
      "Laterite_Soil       0.00      0.00      0.00        35\n",
      "Mountain_Soil       0.18      1.00      0.31        32\n",
      "     Red_Soil       0.99      0.96      0.97       180\n",
      "  Yellow_Soil       0.99      0.82      0.90       224\n",
      "\n",
      "     accuracy                           0.76       816\n",
      "    macro avg       0.64      0.63      0.57       816\n",
      " weighted avg       0.85      0.76      0.77       816\n",
      "\n",
      "\n",
      "‚úì mobilenetv2 training complete!\n",
      "\n",
      "================================================================================\n",
      "[2/3] TRAINING: EFFICIENTNETB0\n",
      "================================================================================\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "\u001b[1m16705208/16705208\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "‚úì Model created: 4,845,738 parameters\n",
      "\n",
      "======================================================================\n",
      "Training efficientnetb0\n",
      "======================================================================\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1760269069.704521      62 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1760269069.894071      62 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1760269070.390197      62 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1760269070.596302      62 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1760269070.971085      62 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1760269071.177272      62 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.6324 - loss: 1.1823 - top3_accuracy: 0.8228"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1760269123.575316      63 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1760269123.765331      63 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1760269124.246386      63 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1760269124.455364      63 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1760269124.828607      63 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1760269125.037527      63 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525ms/step - accuracy: 0.6337 - loss: 1.1790 - top3_accuracy: 0.8238\n",
      "Epoch 1: val_accuracy improved from -inf to 0.23897, saving model to best_efficientnetb0.h5\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 714ms/step - accuracy: 0.6350 - loss: 1.1758 - top3_accuracy: 0.8248 - val_accuracy: 0.2390 - val_loss: 2.4285 - val_top3_accuracy: 0.4326 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8661 - loss: 0.5333 - top3_accuracy: 0.9683\n",
      "Epoch 2: val_accuracy improved from 0.23897 to 0.29167, saving model to best_efficientnetb0.h5\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 135ms/step - accuracy: 0.8662 - loss: 0.5328 - top3_accuracy: 0.9683 - val_accuracy: 0.2917 - val_loss: 2.1198 - val_top3_accuracy: 0.5196 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9203 - loss: 0.2914 - top3_accuracy: 0.9845\n",
      "Epoch 3: val_accuracy did not improve from 0.29167\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 126ms/step - accuracy: 0.9204 - loss: 0.2915 - top3_accuracy: 0.9845 - val_accuracy: 0.1360 - val_loss: 2.3984 - val_top3_accuracy: 0.6213 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9388 - loss: 0.1993 - top3_accuracy: 0.9933\n",
      "Epoch 4: val_accuracy did not improve from 0.29167\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 125ms/step - accuracy: 0.9388 - loss: 0.1996 - top3_accuracy: 0.9933 - val_accuracy: 0.1017 - val_loss: 2.6062 - val_top3_accuracy: 0.3444 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9562 - loss: 0.1724 - top3_accuracy: 0.9915\n",
      "Epoch 5: val_accuracy did not improve from 0.29167\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 125ms/step - accuracy: 0.9563 - loss: 0.1723 - top3_accuracy: 0.9915 - val_accuracy: 0.1716 - val_loss: 3.9904 - val_top3_accuracy: 0.6336 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9537 - loss: 0.1805 - top3_accuracy: 0.9947\n",
      "Epoch 6: val_accuracy improved from 0.29167 to 0.46201, saving model to best_efficientnetb0.h5\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 135ms/step - accuracy: 0.9536 - loss: 0.1811 - top3_accuracy: 0.9946 - val_accuracy: 0.4620 - val_loss: 1.4360 - val_top3_accuracy: 0.7794 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9150 - loss: 0.3023 - top3_accuracy: 0.9914\n",
      "Epoch 7: val_accuracy did not improve from 0.46201\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 125ms/step - accuracy: 0.9151 - loss: 0.3024 - top3_accuracy: 0.9914 - val_accuracy: 0.1887 - val_loss: 6.9611 - val_top3_accuracy: 0.4191 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9469 - loss: 0.1645 - top3_accuracy: 0.9937\n",
      "Epoch 8: val_accuracy did not improve from 0.46201\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.9470 - loss: 0.1644 - top3_accuracy: 0.9937 - val_accuracy: 0.3493 - val_loss: 4.8420 - val_top3_accuracy: 0.5588 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9614 - loss: 0.1268 - top3_accuracy: 0.9935\n",
      "Epoch 9: val_accuracy did not improve from 0.46201\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 126ms/step - accuracy: 0.9614 - loss: 0.1269 - top3_accuracy: 0.9935 - val_accuracy: 0.2267 - val_loss: 2.3333 - val_top3_accuracy: 0.4326 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9687 - loss: 0.1346 - top3_accuracy: 0.9964\n",
      "Epoch 10: val_accuracy did not improve from 0.46201\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 125ms/step - accuracy: 0.9687 - loss: 0.1348 - top3_accuracy: 0.9964 - val_accuracy: 0.0613 - val_loss: 5.0007 - val_top3_accuracy: 0.3113 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9596 - loss: 0.1218 - top3_accuracy: 0.9954\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.46201 to 0.63848, saving model to best_efficientnetb0.h5\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 134ms/step - accuracy: 0.9597 - loss: 0.1218 - top3_accuracy: 0.9954 - val_accuracy: 0.6385 - val_loss: 1.7311 - val_top3_accuracy: 0.8787 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9777 - loss: 0.0738 - top3_accuracy: 0.9983\n",
      "Epoch 12: val_accuracy did not improve from 0.63848\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 126ms/step - accuracy: 0.9778 - loss: 0.0738 - top3_accuracy: 0.9983 - val_accuracy: 0.2953 - val_loss: 2.8684 - val_top3_accuracy: 0.7439 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9895 - loss: 0.0527 - top3_accuracy: 0.9984\n",
      "Epoch 13: val_accuracy improved from 0.63848 to 0.69363, saving model to best_efficientnetb0.h5\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 134ms/step - accuracy: 0.9895 - loss: 0.0526 - top3_accuracy: 0.9984 - val_accuracy: 0.6936 - val_loss: 1.0285 - val_top3_accuracy: 0.9645 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9919 - loss: 0.0289 - top3_accuracy: 0.9989\n",
      "Epoch 14: val_accuracy improved from 0.69363 to 0.72059, saving model to best_efficientnetb0.h5\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 133ms/step - accuracy: 0.9920 - loss: 0.0289 - top3_accuracy: 0.9989 - val_accuracy: 0.7206 - val_loss: 1.2916 - val_top3_accuracy: 0.8615 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9938 - loss: 0.0179 - top3_accuracy: 0.9984\n",
      "Epoch 15: val_accuracy improved from 0.72059 to 0.93260, saving model to best_efficientnetb0.h5\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 135ms/step - accuracy: 0.9938 - loss: 0.0178 - top3_accuracy: 0.9984 - val_accuracy: 0.9326 - val_loss: 0.2903 - val_top3_accuracy: 0.9890 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9954 - loss: 0.0113 - top3_accuracy: 1.0000\n",
      "Epoch 16: val_accuracy did not improve from 0.93260\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 126ms/step - accuracy: 0.9955 - loss: 0.0112 - top3_accuracy: 1.0000 - val_accuracy: 0.9301 - val_loss: 0.2638 - val_top3_accuracy: 0.9951 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9955 - loss: 0.0093 - top3_accuracy: 1.0000\n",
      "Epoch 17: val_accuracy did not improve from 0.93260\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 125ms/step - accuracy: 0.9955 - loss: 0.0092 - top3_accuracy: 1.0000 - val_accuracy: 0.9314 - val_loss: 0.3203 - val_top3_accuracy: 0.9902 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9989 - loss: 0.0050 - top3_accuracy: 1.0000\n",
      "Epoch 18: val_accuracy improved from 0.93260 to 0.97426, saving model to best_efficientnetb0.h5\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 134ms/step - accuracy: 0.9989 - loss: 0.0050 - top3_accuracy: 1.0000 - val_accuracy: 0.9743 - val_loss: 0.1052 - val_top3_accuracy: 0.9963 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9956 - loss: 0.0056 - top3_accuracy: 1.0000\n",
      "Epoch 19: val_accuracy did not improve from 0.97426\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.9956 - loss: 0.0056 - top3_accuracy: 1.0000 - val_accuracy: 0.7132 - val_loss: 1.1305 - val_top3_accuracy: 0.9069 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9943 - loss: 0.0178 - top3_accuracy: 1.0000\n",
      "Epoch 20: val_accuracy did not improve from 0.97426\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 125ms/step - accuracy: 0.9943 - loss: 0.0178 - top3_accuracy: 1.0000 - val_accuracy: 0.6703 - val_loss: 1.6741 - val_top3_accuracy: 0.9571 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9983 - loss: 0.0052 - top3_accuracy: 1.0000\n",
      "Epoch 21: val_accuracy did not improve from 0.97426\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 125ms/step - accuracy: 0.9983 - loss: 0.0052 - top3_accuracy: 1.0000 - val_accuracy: 0.8836 - val_loss: 0.4999 - val_top3_accuracy: 0.9865 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9965 - loss: 0.0080 - top3_accuracy: 1.0000\n",
      "Epoch 22: val_accuracy did not improve from 0.97426\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 125ms/step - accuracy: 0.9965 - loss: 0.0079 - top3_accuracy: 1.0000 - val_accuracy: 0.5429 - val_loss: 1.9170 - val_top3_accuracy: 0.9498 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9943 - loss: 0.0213 - top3_accuracy: 0.9999\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.97426\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 125ms/step - accuracy: 0.9942 - loss: 0.0215 - top3_accuracy: 0.9999 - val_accuracy: 0.1017 - val_loss: 4.1541 - val_top3_accuracy: 0.5331 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9867 - loss: 0.0439 - top3_accuracy: 0.9975\n",
      "Epoch 24: val_accuracy did not improve from 0.97426\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 125ms/step - accuracy: 0.9867 - loss: 0.0438 - top3_accuracy: 0.9975 - val_accuracy: 0.4363 - val_loss: 2.1109 - val_top3_accuracy: 0.6703 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9957 - loss: 0.0257 - top3_accuracy: 0.9993\n",
      "Epoch 25: val_accuracy did not improve from 0.97426\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 125ms/step - accuracy: 0.9957 - loss: 0.0256 - top3_accuracy: 0.9993 - val_accuracy: 0.8260 - val_loss: 0.6777 - val_top3_accuracy: 0.9706 - learning_rate: 2.5000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9968 - loss: 0.0063 - top3_accuracy: 1.0000\n",
      "Epoch 26: val_accuracy did not improve from 0.97426\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 125ms/step - accuracy: 0.9969 - loss: 0.0063 - top3_accuracy: 1.0000 - val_accuracy: 0.9142 - val_loss: 0.3939 - val_top3_accuracy: 0.9914 - learning_rate: 2.5000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9986 - loss: 0.0059 - top3_accuracy: 1.0000\n",
      "Epoch 27: val_accuracy did not improve from 0.97426\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 126ms/step - accuracy: 0.9986 - loss: 0.0059 - top3_accuracy: 1.0000 - val_accuracy: 0.9669 - val_loss: 0.1540 - val_top3_accuracy: 0.9963 - learning_rate: 2.5000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9977 - loss: 0.0033 - top3_accuracy: 1.0000\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.97426\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 125ms/step - accuracy: 0.9977 - loss: 0.0033 - top3_accuracy: 1.0000 - val_accuracy: 0.9142 - val_loss: 0.3776 - val_top3_accuracy: 0.9939 - learning_rate: 2.5000e-04\n",
      "Epoch 28: early stopping\n",
      "Restoring model weights from the end of the best epoch: 18.\n",
      "\n",
      "======================================================================\n",
      "EVALUATION RESULTS: efficientnetb0\n",
      "======================================================================\n",
      "Accuracy: 0.9743\n",
      "Precision: 0.9743\n",
      "Recall: 0.9743\n",
      "F1-Score: 0.9735\n",
      "Cohen's Kappa: 0.9677\n",
      "Matthews Correlation: 0.9678\n",
      "ROC AUC: 0.9995\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Alluvial_Soil       0.94      0.98      0.96       111\n",
      "    Arid_Soil       0.88      0.93      0.91        46\n",
      "   Black_Soil       1.00      1.00      1.00       188\n",
      "Laterite_Soil       0.93      0.71      0.81        35\n",
      "Mountain_Soil       0.97      0.91      0.94        32\n",
      "     Red_Soil       0.99      0.99      0.99       180\n",
      "  Yellow_Soil       0.99      0.99      0.99       224\n",
      "\n",
      "     accuracy                           0.97       816\n",
      "    macro avg       0.96      0.93      0.94       816\n",
      " weighted avg       0.97      0.97      0.97       816\n",
      "\n",
      "\n",
      "‚úì efficientnetb0 training complete!\n",
      "\n",
      "================================================================================\n",
      "[3/3] TRAINING: RESNET50\n",
      "================================================================================\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "‚úì Model created: 24,780,167 parameters\n",
      "\n",
      "======================================================================\n",
      "Training resnet50\n",
      "======================================================================\n",
      "Epoch 1/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 449ms/step - accuracy: 0.6068 - loss: 1.3427 - top3_accuracy: 0.8188\n",
      "Epoch 1: val_accuracy improved from -inf to 0.04289, saving model to best_resnet50.h5\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 580ms/step - accuracy: 0.6078 - loss: 1.3402 - top3_accuracy: 0.8196 - val_accuracy: 0.0429 - val_loss: 10.6875 - val_top3_accuracy: 0.3039 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.7882 - loss: 0.7728 - top3_accuracy: 0.9446\n",
      "Epoch 2: val_accuracy improved from 0.04289 to 0.27451, saving model to best_resnet50.h5\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 191ms/step - accuracy: 0.7884 - loss: 0.7728 - top3_accuracy: 0.9447 - val_accuracy: 0.2745 - val_loss: 46.1275 - val_top3_accuracy: 0.6311 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8176 - loss: 0.7053 - top3_accuracy: 0.9650\n",
      "Epoch 3: val_accuracy did not improve from 0.27451\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 176ms/step - accuracy: 0.8177 - loss: 0.7050 - top3_accuracy: 0.9650 - val_accuracy: 0.2353 - val_loss: 2.7639 - val_top3_accuracy: 0.4596 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8579 - loss: 0.5399 - top3_accuracy: 0.9759\n",
      "Epoch 4: val_accuracy did not improve from 0.27451\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 176ms/step - accuracy: 0.8579 - loss: 0.5400 - top3_accuracy: 0.9758 - val_accuracy: 0.0564 - val_loss: 14.9963 - val_top3_accuracy: 0.6409 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8788 - loss: 0.4760 - top3_accuracy: 0.9786\n",
      "Epoch 5: val_accuracy did not improve from 0.27451\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 174ms/step - accuracy: 0.8789 - loss: 0.4760 - top3_accuracy: 0.9786 - val_accuracy: 0.2598 - val_loss: 3.0401 - val_top3_accuracy: 0.4828 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8960 - loss: 0.4224 - top3_accuracy: 0.9790\n",
      "Epoch 6: val_accuracy improved from 0.27451 to 0.31005, saving model to best_resnet50.h5\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 189ms/step - accuracy: 0.8959 - loss: 0.4227 - top3_accuracy: 0.9790 - val_accuracy: 0.3100 - val_loss: 4.3571 - val_top3_accuracy: 0.5711 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8994 - loss: 0.4117 - top3_accuracy: 0.9793\n",
      "Epoch 7: val_accuracy improved from 0.31005 to 0.73162, saving model to best_resnet50.h5\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step - accuracy: 0.8994 - loss: 0.4116 - top3_accuracy: 0.9793 - val_accuracy: 0.7316 - val_loss: 1.0082 - val_top3_accuracy: 0.9326 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9119 - loss: 0.3437 - top3_accuracy: 0.9870\n",
      "Epoch 8: val_accuracy improved from 0.73162 to 0.83211, saving model to best_resnet50.h5\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 189ms/step - accuracy: 0.9120 - loss: 0.3436 - top3_accuracy: 0.9870 - val_accuracy: 0.8321 - val_loss: 1.1165 - val_top3_accuracy: 0.9755 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8960 - loss: 0.5321 - top3_accuracy: 0.9733\n",
      "Epoch 9: val_accuracy did not improve from 0.83211\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 175ms/step - accuracy: 0.8960 - loss: 0.5319 - top3_accuracy: 0.9734 - val_accuracy: 0.5686 - val_loss: 175.3574 - val_top3_accuracy: 0.9400 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8697 - loss: 0.4814 - top3_accuracy: 0.9777\n",
      "Epoch 10: val_accuracy did not improve from 0.83211\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 176ms/step - accuracy: 0.8697 - loss: 0.4814 - top3_accuracy: 0.9777 - val_accuracy: 0.7047 - val_loss: 0.8318 - val_top3_accuracy: 0.9522 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9154 - loss: 0.3682 - top3_accuracy: 0.9869\n",
      "Epoch 11: val_accuracy improved from 0.83211 to 0.87623, saving model to best_resnet50.h5\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step - accuracy: 0.9154 - loss: 0.3679 - top3_accuracy: 0.9870 - val_accuracy: 0.8762 - val_loss: 0.3971 - val_top3_accuracy: 0.9779 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9171 - loss: 0.3179 - top3_accuracy: 0.9906\n",
      "Epoch 12: val_accuracy improved from 0.87623 to 0.91054, saving model to best_resnet50.h5\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 190ms/step - accuracy: 0.9171 - loss: 0.3178 - top3_accuracy: 0.9907 - val_accuracy: 0.9105 - val_loss: 0.3513 - val_top3_accuracy: 0.9926 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.9406 - loss: 0.2246 - top3_accuracy: 0.9945\n",
      "Epoch 13: val_accuracy improved from 0.91054 to 0.92402, saving model to best_resnet50.h5\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step - accuracy: 0.9406 - loss: 0.2245 - top3_accuracy: 0.9945 - val_accuracy: 0.9240 - val_loss: 0.2437 - val_top3_accuracy: 0.9853 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9579 - loss: 0.1738 - top3_accuracy: 0.9947\n",
      "Epoch 14: val_accuracy did not improve from 0.92402\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 174ms/step - accuracy: 0.9579 - loss: 0.1737 - top3_accuracy: 0.9947 - val_accuracy: 0.8958 - val_loss: 0.3651 - val_top3_accuracy: 0.9841 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.9469 - loss: 0.1846 - top3_accuracy: 0.9954\n",
      "Epoch 15: val_accuracy improved from 0.92402 to 0.92525, saving model to best_resnet50.h5\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 189ms/step - accuracy: 0.9469 - loss: 0.1845 - top3_accuracy: 0.9954 - val_accuracy: 0.9252 - val_loss: 0.2436 - val_top3_accuracy: 0.9914 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9674 - loss: 0.1146 - top3_accuracy: 0.9994\n",
      "Epoch 16: val_accuracy did not improve from 0.92525\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 174ms/step - accuracy: 0.9674 - loss: 0.1145 - top3_accuracy: 0.9994 - val_accuracy: 0.9179 - val_loss: 0.6942 - val_top3_accuracy: 0.9853 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9749 - loss: 0.0871 - top3_accuracy: 0.9984\n",
      "Epoch 17: val_accuracy did not improve from 0.92525\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 174ms/step - accuracy: 0.9749 - loss: 0.0872 - top3_accuracy: 0.9984 - val_accuracy: 0.9179 - val_loss: 0.2748 - val_top3_accuracy: 0.9865 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9732 - loss: 0.0926 - top3_accuracy: 0.9963\n",
      "Epoch 18: val_accuracy did not improve from 0.92525\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 174ms/step - accuracy: 0.9732 - loss: 0.0926 - top3_accuracy: 0.9963 - val_accuracy: 0.8297 - val_loss: 0.8683 - val_top3_accuracy: 0.9877 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9472 - loss: 0.1631 - top3_accuracy: 0.9972\n",
      "Epoch 19: val_accuracy did not improve from 0.92525\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 174ms/step - accuracy: 0.9473 - loss: 0.1629 - top3_accuracy: 0.9972 - val_accuracy: 0.9130 - val_loss: 0.3527 - val_top3_accuracy: 0.9939 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9727 - loss: 0.1054 - top3_accuracy: 0.9983\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.92525\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 174ms/step - accuracy: 0.9726 - loss: 0.1055 - top3_accuracy: 0.9983 - val_accuracy: 0.8603 - val_loss: 0.5702 - val_top3_accuracy: 0.9645 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9552 - loss: 0.1528 - top3_accuracy: 0.9966\n",
      "Epoch 21: val_accuracy improved from 0.92525 to 0.94118, saving model to best_resnet50.h5\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 189ms/step - accuracy: 0.9552 - loss: 0.1526 - top3_accuracy: 0.9966 - val_accuracy: 0.9412 - val_loss: 0.2242 - val_top3_accuracy: 0.9902 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9743 - loss: 0.0786 - top3_accuracy: 0.9983\n",
      "Epoch 22: val_accuracy improved from 0.94118 to 0.95711, saving model to best_resnet50.h5\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 189ms/step - accuracy: 0.9744 - loss: 0.0785 - top3_accuracy: 0.9984 - val_accuracy: 0.9571 - val_loss: 0.1438 - val_top3_accuracy: 0.9963 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9915 - loss: 0.0293 - top3_accuracy: 1.0000\n",
      "Epoch 23: val_accuracy did not improve from 0.95711\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 173ms/step - accuracy: 0.9915 - loss: 0.0292 - top3_accuracy: 1.0000 - val_accuracy: 0.9522 - val_loss: 0.1711 - val_top3_accuracy: 0.9951 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.9944 - loss: 0.0171 - top3_accuracy: 1.0000\n",
      "Epoch 24: val_accuracy improved from 0.95711 to 0.96078, saving model to best_resnet50.h5\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 188ms/step - accuracy: 0.9945 - loss: 0.0170 - top3_accuracy: 1.0000 - val_accuracy: 0.9608 - val_loss: 0.1391 - val_top3_accuracy: 0.9963 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.9971 - loss: 0.0081 - top3_accuracy: 1.0000\n",
      "Epoch 25: val_accuracy did not improve from 0.96078\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 173ms/step - accuracy: 0.9972 - loss: 0.0081 - top3_accuracy: 1.0000 - val_accuracy: 0.9559 - val_loss: 0.1765 - val_top3_accuracy: 0.9951 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.9961 - loss: 0.0129 - top3_accuracy: 1.0000\n",
      "Epoch 26: val_accuracy did not improve from 0.96078\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 173ms/step - accuracy: 0.9961 - loss: 0.0128 - top3_accuracy: 1.0000 - val_accuracy: 0.9583 - val_loss: 0.1785 - val_top3_accuracy: 0.9939 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9971 - loss: 0.0054 - top3_accuracy: 1.0000\n",
      "Epoch 27: val_accuracy did not improve from 0.96078\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 174ms/step - accuracy: 0.9971 - loss: 0.0054 - top3_accuracy: 1.0000 - val_accuracy: 0.9596 - val_loss: 0.1613 - val_top3_accuracy: 0.9951 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.9984 - loss: 0.0034 - top3_accuracy: 1.0000\n",
      "Epoch 28: val_accuracy improved from 0.96078 to 0.96936, saving model to best_resnet50.h5\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 189ms/step - accuracy: 0.9984 - loss: 0.0034 - top3_accuracy: 1.0000 - val_accuracy: 0.9694 - val_loss: 0.1572 - val_top3_accuracy: 0.9951 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.9983 - loss: 0.0073 - top3_accuracy: 1.0000\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.96936\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 174ms/step - accuracy: 0.9983 - loss: 0.0073 - top3_accuracy: 1.0000 - val_accuracy: 0.9620 - val_loss: 0.1512 - val_top3_accuracy: 0.9975 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9996 - loss: 0.0035 - top3_accuracy: 1.0000\n",
      "Epoch 30: val_accuracy did not improve from 0.96936\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 174ms/step - accuracy: 0.9996 - loss: 0.0035 - top3_accuracy: 1.0000 - val_accuracy: 0.9583 - val_loss: 0.1663 - val_top3_accuracy: 0.9951 - learning_rate: 2.5000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9986 - loss: 0.0035 - top3_accuracy: 1.0000\n",
      "Epoch 31: val_accuracy did not improve from 0.96936\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 174ms/step - accuracy: 0.9986 - loss: 0.0035 - top3_accuracy: 1.0000 - val_accuracy: 0.9596 - val_loss: 0.1721 - val_top3_accuracy: 0.9926 - learning_rate: 2.5000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.9998 - loss: 0.0014 - top3_accuracy: 1.0000\n",
      "Epoch 32: val_accuracy did not improve from 0.96936\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 174ms/step - accuracy: 0.9998 - loss: 0.0014 - top3_accuracy: 1.0000 - val_accuracy: 0.9596 - val_loss: 0.1760 - val_top3_accuracy: 0.9939 - learning_rate: 2.5000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 0.0011 - top3_accuracy: 1.0000\n",
      "Epoch 33: val_accuracy did not improve from 0.96936\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 174ms/step - accuracy: 1.0000 - loss: 0.0011 - top3_accuracy: 1.0000 - val_accuracy: 0.9620 - val_loss: 0.1696 - val_top3_accuracy: 0.9951 - learning_rate: 2.5000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 9.7854e-04 - top3_accuracy: 1.0000\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.96936\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 174ms/step - accuracy: 1.0000 - loss: 9.7788e-04 - top3_accuracy: 1.0000 - val_accuracy: 0.9608 - val_loss: 0.1715 - val_top3_accuracy: 0.9939 - learning_rate: 2.5000e-04\n",
      "Epoch 34: early stopping\n",
      "Restoring model weights from the end of the best epoch: 24.\n",
      "\n",
      "======================================================================\n",
      "EVALUATION RESULTS: resnet50\n",
      "======================================================================\n",
      "Accuracy: 0.9608\n",
      "Precision: 0.9622\n",
      "Recall: 0.9608\n",
      "F1-Score: 0.9614\n",
      "Cohen's Kappa: 0.9510\n",
      "Matthews Correlation: 0.9510\n",
      "ROC AUC: 0.9985\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Alluvial_Soil       0.94      0.96      0.95       111\n",
      "    Arid_Soil       0.80      0.85      0.82        46\n",
      "   Black_Soil       0.99      0.97      0.98       188\n",
      "Laterite_Soil       0.86      0.86      0.86        35\n",
      "Mountain_Soil       0.79      0.84      0.82        32\n",
      "     Red_Soil       0.99      0.99      0.99       180\n",
      "  Yellow_Soil       1.00      0.98      0.99       224\n",
      "\n",
      "     accuracy                           0.96       816\n",
      "    macro avg       0.91      0.92      0.92       816\n",
      " weighted avg       0.96      0.96      0.96       816\n",
      "\n",
      "\n",
      "‚úì resnet50 training complete!\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Model Training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: MODEL TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if MODELS_TO_TRAIN == 'all':\n",
    "    model_list = ['mobilenetv2', 'efficientnetb0', 'resnet50', 'densenet121', 'inceptionv3', 'xception', 'custom_cnn']\n",
    "elif MODELS_TO_TRAIN == 'fast':\n",
    "    model_list = ['mobilenetv2', 'efficientnetb0', 'resnet50']\n",
    "else:\n",
    "    model_list = MODELS_TO_TRAIN\n",
    "\n",
    "print(f\"\\nüìã Models to train: {', '.join(model_list)}\")\n",
    "\n",
    "factory = ModelFactory(input_shape=images.shape[1:], num_classes=len(class_names))\n",
    "trained_models = {}\n",
    "evaluator = ModelEvaluator(class_names)\n",
    "\n",
    "for idx, model_name in enumerate(model_list, 1):\n",
    "    try:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"[{idx}/{len(model_list)}] TRAINING: {model_name.upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        model = factory.create_model(model_name, pretrained=True)\n",
    "        print(f\"‚úì Model created: {model.count_params():,} parameters\")\n",
    "        \n",
    "        trainer = ModelTrainer(model, model_name, X_train, y_train, X_val, y_val, class_weights)\n",
    "        trainer.compile_model(optimizer=CONFIG['optimizer'], learning_rate=CONFIG['learning_rate'])\n",
    "        trainer.train(epochs=CONFIG['epochs'], batch_size=CONFIG['batch_size'], patience=CONFIG['patience'])\n",
    "        \n",
    "        results = trainer.evaluate()\n",
    "        evaluator.evaluate_model(model_name, y_val, results['predictions'],\n",
    "                                results['probabilities'], results['history'])\n",
    "        \n",
    "        evaluator.print_results(model_name)\n",
    "        evaluator.plot_confusion_matrix(model_name)\n",
    "        evaluator.plot_training_history(model_name)\n",
    "        \n",
    "        trained_models[model_name] = model\n",
    "        print(f\"\\n‚úì {model_name} training complete!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error training {model_name}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "530f426f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T11:58:51.269232Z",
     "iopub.status.busy": "2025-10-12T11:58:51.268362Z",
     "iopub.status.idle": "2025-10-12T11:58:59.574667Z",
     "shell.execute_reply": "2025-10-12T11:58:59.573778Z"
    },
    "papermill": {
     "duration": 8.723384,
     "end_time": "2025-10-12T11:58:59.576187",
     "exception": false,
     "start_time": "2025-10-12T11:58:50.852803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 5: ENSEMBLE METHODS\n",
      "================================================================================\n",
      "\n",
      "======================================================================\n",
      "EVALUATION RESULTS: Ensemble_Voting\n",
      "======================================================================\n",
      "Accuracy: 0.9779\n",
      "Precision: 0.9795\n",
      "Recall: 0.9779\n",
      "F1-Score: 0.9778\n",
      "Cohen's Kappa: 0.9724\n",
      "Matthews Correlation: 0.9725\n",
      "ROC AUC: 0.9993\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Alluvial_Soil       0.97      0.98      0.98       111\n",
      "    Arid_Soil       0.93      0.89      0.91        46\n",
      "   Black_Soil       1.00      0.99      1.00       188\n",
      "Laterite_Soil       0.96      0.77      0.86        35\n",
      "Mountain_Soil       0.80      1.00      0.89        32\n",
      "     Red_Soil       0.99      1.00      0.99       180\n",
      "  Yellow_Soil       1.00      0.99      0.99       224\n",
      "\n",
      "     accuracy                           0.98       816\n",
      "    macro avg       0.95      0.95      0.95       816\n",
      " weighted avg       0.98      0.98      0.98       816\n",
      "\n",
      "‚úì Ensemble evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Ensemble (if multiple models)\n",
    "if len(trained_models) >= 2:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 5: ENSEMBLE METHODS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    ensemble = EnsembleModel(list(trained_models.values()), list(trained_models.keys()))\n",
    "    y_pred_ensemble, y_pred_probs_ensemble = ensemble.predict_voting(X_val, method='soft')\n",
    "    evaluator.evaluate_model('Ensemble_Voting', y_val, y_pred_ensemble, y_pred_probs_ensemble)\n",
    "    evaluator.print_results('Ensemble_Voting')\n",
    "    evaluator.plot_confusion_matrix('Ensemble_Voting')\n",
    "    print(\"‚úì Ensemble evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecaad3ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T11:59:00.402574Z",
     "iopub.status.busy": "2025-10-12T11:59:00.402273Z",
     "iopub.status.idle": "2025-10-12T11:59:00.410650Z",
     "shell.execute_reply": "2025-10-12T11:59:00.409798Z"
    },
    "papermill": {
     "duration": 0.420364,
     "end_time": "2025-10-12T11:59:00.412021",
     "exception": false,
     "start_time": "2025-10-12T11:58:59.991657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 6: FINAL TEST SET EVALUATION\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Test Set Evaluation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: FINAL TEST SET EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if trained_models:\n",
    "    best_model_name = max(evaluator.results.items(), key=lambda x: x[1]['accuracy'])[0]\n",
    "    best_model = trained_models.get(best_model_name)\n",
    "    \n",
    "    if best_model:\n",
    "        print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "        \n",
    "        y_test_pred_probs = best_model.predict(X_test, verbose=0)\n",
    "        y_test_pred = np.argmax(y_test_pred_probs, axis=1)\n",
    "        \n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        print(f\"üìä Test Accuracy: {test_accuracy:.4f}\")\n",
    "        print(f\"\\n{classification_report(y_test, y_test_pred, target_names=class_names, zero_division=0)}\")\n",
    "        \n",
    "        # Test confusion matrix\n",
    "        cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title(f'Test Confusion Matrix - {best_model_name}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('test_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "else:\n",
    "    print(\"\\n‚ö† No models were successfully trained\")\n",
    "    best_model = None\n",
    "    best_model_name = \"None\"\n",
    "    test_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b72ae1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T11:59:01.367823Z",
     "iopub.status.busy": "2025-10-12T11:59:01.367042Z",
     "iopub.status.idle": "2025-10-12T11:59:03.077767Z",
     "shell.execute_reply": "2025-10-12T11:59:03.076825Z"
    },
    "papermill": {
     "duration": 2.130196,
     "end_time": "2025-10-12T11:59:03.079444",
     "exception": false,
     "start_time": "2025-10-12T11:59:00.949248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 7: MODEL COMPARISON\n",
      "================================================================================\n",
      "\n",
      "======================================================================\n",
      "MODEL COMPARISON\n",
      "======================================================================\n",
      "          Model  Accuracy  Precision   Recall  F1-Score  Cohen Kappa  Matthews Corr\n",
      "Ensemble_Voting  0.977941   0.979483 0.977941  0.977779     0.972353       0.972467\n",
      " efficientnetb0  0.974265   0.974307 0.974265  0.973544     0.967698       0.967794\n",
      "       resnet50  0.960784   0.962216 0.960784  0.961353     0.950951       0.951001\n",
      "    mobilenetv2  0.764706   0.851243 0.764706  0.773751     0.710675       0.729998\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 7: MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_df = evaluator.compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c0d6207",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T11:59:03.904215Z",
     "iopub.status.busy": "2025-10-12T11:59:03.903495Z",
     "iopub.status.idle": "2025-10-12T11:59:03.910440Z",
     "shell.execute_reply": "2025-10-12T11:59:03.909683Z"
    },
    "papermill": {
     "duration": 0.41916,
     "end_time": "2025-10-12T11:59:03.911822",
     "exception": false,
     "start_time": "2025-10-12T11:59:03.492662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 8: SAVING RESULTS\n",
      "================================================================================\n",
      "‚úì Saved: results_summary.json\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Save Results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 8: SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if trained_models and best_model:\n",
    "    best_model.save(f'final_best_model_{best_model_name}.h5')\n",
    "    print(f\"‚úì Saved: final_best_model_{best_model_name}.h5\")\n",
    "\n",
    "results_summary = {\n",
    "    'best_model': best_model_name if trained_models else 'None',\n",
    "    'test_accuracy': float(test_accuracy) if trained_models and best_model else 0.0,\n",
    "    'configuration': CONFIG,\n",
    "    'class_names': class_names,\n",
    "    'num_classes': len(class_names),\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "with open('results_summary.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=4)\n",
    "print(\"‚úì Saved: results_summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f59d63cf",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-12T11:59:04.751932Z",
     "iopub.status.busy": "2025-10-12T11:59:04.751295Z",
     "iopub.status.idle": "2025-10-12T11:59:04.757420Z",
     "shell.execute_reply": "2025-10-12T11:59:04.756489Z"
    },
    "papermill": {
     "duration": 0.424827,
     "end_time": "2025-10-12T11:59:04.758851",
     "exception": false,
     "start_time": "2025-10-12T11:59:04.334024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚úÖ PIPELINE COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "‚ö† No models were successfully trained. Check errors above.\n",
      "üìÅ Available outputs in: /kaggle/working/soil_classification_results/soil_classification_results\n"
     ]
    }
   ],
   "source": [
    "# Final Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ PIPELINE COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "if trained_models and best_model:\n",
    "    print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "    print(f\"üìä Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"\\nüìÅ All results saved in: {output_dir.absolute()}\")\n",
    "else:\n",
    "    print(\"\\n‚ö† No models were successfully trained. Check errors above.\")\n",
    "    print(f\"üìÅ Available outputs in: {output_dir.absolute()}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7647147,
     "sourceId": 12142148,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1793.883413,
   "end_time": "2025-10-12T11:59:08.602852",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-12T11:29:14.719439",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
