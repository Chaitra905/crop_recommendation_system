{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7657925",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-19T10:51:53.262658Z",
     "iopub.status.busy": "2025-06-19T10:51:53.261957Z",
     "iopub.status.idle": "2025-06-19T10:51:54.815837Z",
     "shell.execute_reply": "2025-06-19T10:51:54.815305Z"
    },
    "papermill": {
     "duration": 1.560647,
     "end_time": "2025-06-19T10:51:54.817300",
     "exception": false,
     "start_time": "2025-06-19T10:51:53.256653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6421503",
   "metadata": {
    "papermill": {
     "duration": 0.003035,
     "end_time": "2025-06-19T10:51:54.824050",
     "exception": false,
     "start_time": "2025-06-19T10:51:54.821015",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Soil Classification using Tensorflow \n",
    "For this first i am going to try with the meticulously preprocessed dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bc41e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:51:54.831041Z",
     "iopub.status.busy": "2025-06-19T10:51:54.830731Z",
     "iopub.status.idle": "2025-06-19T10:52:10.446944Z",
     "shell.execute_reply": "2025-06-19T10:52:10.446355Z"
    },
    "papermill": {
     "duration": 15.621189,
     "end_time": "2025-06-19T10:52:10.448377",
     "exception": false,
     "start_time": "2025-06-19T10:51:54.827188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 10:51:56.794694: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750330317.054346      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750330317.121900      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# importing modules \n",
    "import tensorflow as tf \n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt \n",
    "from tensorflow.keras.layers import Rescaling , Dense , MaxPooling2D ,Conv2D , Input , Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy as scc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10da0b00",
   "metadata": {
    "papermill": {
     "duration": 0.003051,
     "end_time": "2025-06-19T10:52:10.454902",
     "exception": false,
     "start_time": "2025-06-19T10:52:10.451851",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "TO check if the pixel size of the generated dataset are of consistent size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f83309",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:52:10.462517Z",
     "iopub.status.busy": "2025-06-19T10:52:10.461880Z",
     "iopub.status.idle": "2025-06-19T10:52:10.516058Z",
     "shell.execute_reply": "2025-06-19T10:52:10.515304Z"
    },
    "papermill": {
     "duration": 0.059093,
     "end_time": "2025-06-19T10:52:10.517307",
     "exception": false,
     "start_time": "2025-06-19T10:52:10.458214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width: 1000 pixels\n",
      "Height: 544 pixels\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with Image.open(\"/kaggle/input/comprehensive-soil-classification-datasets/Orignal-Dataset/Black_Soil/1.jpg\") as img :\n",
    "    width , height = img.size\n",
    "    print(f\"Width: {width} pixels\")\n",
    "    print(f\"Height: {height} pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbba1726",
   "metadata": {
    "papermill": {
     "duration": 0.003146,
     "end_time": "2025-06-19T10:52:10.523994",
     "exception": false,
     "start_time": "2025-06-19T10:52:10.520848",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As seen above the sizes are inconsistent \n",
    "\n",
    "Let us load the dataset into tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40587e67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:52:10.531446Z",
     "iopub.status.busy": "2025-06-19T10:52:10.531215Z",
     "iopub.status.idle": "2025-06-19T10:52:10.816745Z",
     "shell.execute_reply": "2025-06-19T10:52:10.815574Z"
    },
    "papermill": {
     "duration": 0.290631,
     "end_time": "2025-06-19T10:52:10.817860",
     "exception": true,
     "start_time": "2025-06-19T10:52:10.527229",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Could not find directory /kaggle/working/CyAUG-Cleaned2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19/3234258400.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mih\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m224\u001b[0m \u001b[0;31m# image height and image widht , so the image is of 224 pixels square\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m train_ds = tf.keras.utils.image_dataset_from_directory(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mnew_dir\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/image_dataset_utils.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     image_paths, labels, class_names = dataset_utils.index_directory(\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"inferred\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0msubdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    766\u001b[0m   \"\"\"\n\u001b[1;32m    767\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m     raise errors.NotFoundError(\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0mnode_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Could not find directory /kaggle/working/CyAUG-Cleaned2"
     ]
    }
   ],
   "source": [
    "# let us load the modeles uing keras moudule image_datasets_from_directory \n",
    "dir = \"/kaggle/input/comprehensive-soil-classification-datasets/CyAUG-Dataset\"\n",
    "new_dir = \"/kaggle/working/CyAUG-Cleaned2\"\n",
    "ih = iw = 224 # image height and image widht , so the image is of 224 pixels square \n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    new_dir , \n",
    "    validation_split = 0.2 , \n",
    "    subset = 'training' , \n",
    "    seed = 123 , \n",
    "    image_size = (ih , iw) , \n",
    "    batch_size = 32 , \n",
    "    follow_links = True\n",
    ")\n",
    "\n",
    "val_test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  new_dir,\n",
    "  validation_split=0.2, # Same split percentage\n",
    "  subset=\"validation\",\n",
    "  seed=123, # Same seed for consistent split\n",
    "  image_size=(ih, iw),\n",
    "  batch_size = 32 \n",
    ")\n",
    "\n",
    "print(f\"Number of training batches: {len(train_ds)}\")\n",
    "print(f\"Number of validation/test batches: {len(val_test_ds)}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc5e411",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:38:12.246840Z",
     "iopub.status.busy": "2025-06-19T10:38:12.246565Z",
     "iopub.status.idle": "2025-06-19T10:38:12.251139Z",
     "shell.execute_reply": "2025-06-19T10:38:12.250294Z",
     "shell.execute_reply.started": "2025-06-19T10:38:12.246816Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes = train_ds.class_names\n",
    "print(f\"Names of the different types of soil are {classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78270b4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:38:12.548022Z",
     "iopub.status.busy": "2025-06-19T10:38:12.547757Z",
     "iopub.status.idle": "2025-06-19T10:38:12.552087Z",
     "shell.execute_reply": "2025-06-19T10:38:12.551422Z",
     "shell.execute_reply.started": "2025-06-19T10:38:12.548003Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "thala = len(classes) # Thala for a reason # Suiiiiii\n",
    "print(\"Thala \" , thala)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5c188e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:38:17.696359Z",
     "iopub.status.busy": "2025-06-19T10:38:17.696094Z",
     "iopub.status.idle": "2025-06-19T10:38:17.726168Z",
     "shell.execute_reply": "2025-06-19T10:38:17.725638Z",
     "shell.execute_reply.started": "2025-06-19T10:38:17.696341Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Normalization layers used for autotuning such that tensorflow dynamically resizes the batches and stuff for the cache and according to the available memeory \n",
    "N_l = Rescaling(1./255)\n",
    "train_ds = train_ds.map(lambda x, y: (N_l(x), y))\n",
    "val_test_ds = val_test_ds.map(lambda x, y: (N_l(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3827200",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:38:17.990217Z",
     "iopub.status.busy": "2025-06-19T10:38:17.989411Z",
     "iopub.status.idle": "2025-06-19T10:38:18.109196Z",
     "shell.execute_reply": "2025-06-19T10:38:18.108573Z",
     "shell.execute_reply.started": "2025-06-19T10:38:17.990191Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for images, labels in train_ds.take(1):\n",
    "  image_shape = images.shape[1:]\n",
    "  print(f\"Train batch image shape: {images.shape}, labels shape: {labels.shape}\")\n",
    "\n",
    "for images, labels in val_test_ds.take(1):\n",
    "  print(f\"Val/Test batch image shape: {images.shape}, labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e07ef87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:38:18.971558Z",
     "iopub.status.busy": "2025-06-19T10:38:18.970885Z",
     "iopub.status.idle": "2025-06-19T10:38:18.975004Z",
     "shell.execute_reply": "2025-06-19T10:38:18.974420Z",
     "shell.execute_reply.started": "2025-06-19T10:38:18.971534Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb9b37",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "The batch has 32 filters due to the batch sieze and the shape of the image is (224 , 224) and 3 refers to the channels as the image is chromatic and has 4 color channels that are Red , Green and Blue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7308e94f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:47:13.640063Z",
     "iopub.status.busy": "2025-06-19T10:47:13.639804Z",
     "iopub.status.idle": "2025-06-19T10:47:13.682051Z",
     "shell.execute_reply": "2025-06-19T10:47:13.681552Z",
     "shell.execute_reply.started": "2025-06-19T10:47:13.640044Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input(shape = image_shape) ,\n",
    "    # First convolution block \n",
    "    Conv2D(\n",
    "        filters = 32 , \n",
    "        kernel_size = (4 , 4) , \n",
    "        strides = (2, 2) , \n",
    "        activation = 'relu'\n",
    "    ) , \n",
    "    MaxPooling2D(pool_size = (2,2)) , \n",
    "    # Second Convolution Block \n",
    "    Conv2D(\n",
    "        filters = 64 , \n",
    "        kernel_size = (3 , 3) , \n",
    "        strides = (1, 1) , \n",
    "        activation = 'relu'\n",
    "    ) , \n",
    "    MaxPooling2D(pool_size = (2,2)) , \n",
    "    # Classifier block \n",
    "    Flatten() , \n",
    "    Dense(\n",
    "        128 , \n",
    "        activation = 'relu'\n",
    "    ) , \n",
    "    Dense(\n",
    "        64 , \n",
    "        activation = 'relu'\n",
    "    ) , \n",
    "    Dense(\n",
    "        thala , \n",
    "        activation = \"softmax\"\n",
    "    )\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b02de3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:47:15.658896Z",
     "iopub.status.busy": "2025-06-19T10:47:15.658207Z",
     "iopub.status.idle": "2025-06-19T10:47:15.679768Z",
     "shell.execute_reply": "2025-06-19T10:47:15.679103Z",
     "shell.execute_reply.started": "2025-06-19T10:47:15.658873Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = 'adam' , \n",
    "    loss = scc(from_logits = False) , \n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef667175",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:50:49.150951Z",
     "iopub.status.busy": "2025-06-19T10:50:49.150685Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds , \n",
    "    epochs = 100 , \n",
    "    validation_data = val_test_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105ea04e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:50:41.360358Z",
     "iopub.status.busy": "2025-06-19T10:50:41.359641Z",
     "iopub.status.idle": "2025-06-19T10:50:42.146273Z",
     "shell.execute_reply": "2025-06-19T10:50:42.145706Z",
     "shell.execute_reply.started": "2025-06-19T10:50:41.360335Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(val_test_ds, verbose=2)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy * 100:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4faf1de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:23:27.879425Z",
     "iopub.status.busy": "2025-06-19T10:23:27.879137Z",
     "iopub.status.idle": "2025-06-19T10:23:33.953143Z",
     "shell.execute_reply": "2025-06-19T10:23:33.952551Z",
     "shell.execute_reply.started": "2025-06-19T10:23:27.879404Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_dataset_path = '/kaggle/input/comprehensive-soil-classification-datasets/CyAUG-Dataset'\n",
    "\n",
    "# Define the supported image extensions (case-insensitive)\n",
    "supported_image_extensions = ['.jpeg', '.jpg', '.png', '.gif', '.bmp']\n",
    "\n",
    "print(f\"Scanning directory: {base_dataset_path} for unsupported file types...\\n\")\n",
    "\n",
    "unsupported_files_found = False\n",
    "\n",
    "# Walk through all directories and files in the specified path\n",
    "for dirname, _, filenames in os.walk(base_dataset_path):\n",
    "    for filename in filenames:\n",
    "        file_path = os.path.join(dirname, filename)\n",
    "        \n",
    "        # Get the file extension and convert to lowercase for case-insensitive comparison\n",
    "        file_extension = os.path.splitext(filename)[1].lower()\n",
    "\n",
    "        # Check if the file extension is NOT in our list of supported image extensions\n",
    "        if file_extension not in supported_image_extensions:\n",
    "            print(f\"Unsupported file type found: {file_path}\")\n",
    "            unsupported_files_found = True\n",
    "\n",
    "if not unsupported_files_found:\n",
    "    print(\"No unsupported image file extensions (JPEG, PNG, GIF, BMP) found in the dataset.\")\n",
    "    print(\"However, this does not guarantee files are not corrupted or have incorrect headers.\")\n",
    "\n",
    "print(\"\\nScan complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a69fa2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:24:52.898514Z",
     "iopub.status.busy": "2025-06-19T10:24:52.897890Z",
     "iopub.status.idle": "2025-06-19T10:24:57.310135Z",
     "shell.execute_reply": "2025-06-19T10:24:57.309187Z",
     "shell.execute_reply.started": "2025-06-19T10:24:52.898490Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Root directory of your dataset\n",
    "dataset_dir = '/kaggle/input/comprehensive-soil-classification-datasets/CyAUG-Dataset'\n",
    "\n",
    "# Walk through all subdirectories\n",
    "for root, dirs, files in os.walk(dataset_dir):\n",
    "    for file in files:\n",
    "        if file.lower().endswith('.webp'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            print(f\"Removing unsupported file: {file_path}\")\n",
    "            os.remove(file_path)  # or use shutil.move(file_path, target_dir) to move instead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7683178f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:36:17.365818Z",
     "iopub.status.busy": "2025-06-19T10:36:17.365216Z",
     "iopub.status.idle": "2025-06-19T10:36:24.998280Z",
     "shell.execute_reply": "2025-06-19T10:36:24.997633Z",
     "shell.execute_reply.started": "2025-06-19T10:36:17.365796Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_dir = Path(\"/kaggle/input/comprehensive-soil-classification-datasets/CyAUG-Dataset\")\n",
    "dst_dir = Path(\"/kaggle/working/CyAUG-Cleaned2\")\n",
    "\n",
    "# Create the cleaned directory\n",
    "dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Supported extensions\n",
    "supported_exts = ('.jpg', '.jpeg', '.png')\n",
    "\n",
    "# File to skip (relative path from src_dir)\n",
    "skip_file = os.path.join(\"Alluvial_Soil\", \"44.jpg\")\n",
    "\n",
    "# Copy loop\n",
    "for root, dirs, files in os.walk(src_dir):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(supported_exts):\n",
    "            # Compute relative path to check against skip_file\n",
    "            rel_path = os.path.relpath(os.path.join(root, file), src_dir)\n",
    "            if rel_path == skip_file:\n",
    "                print(f\"❌ Skipping file: {rel_path}\")\n",
    "                continue\n",
    "\n",
    "            # Destination\n",
    "            src_file = os.path.join(root, file)\n",
    "            dest_subdir = dst_dir / os.path.relpath(root, src_dir)\n",
    "            dest_subdir.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copy(src_file, dest_subdir / file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90f3292",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:32:56.288511Z",
     "iopub.status.busy": "2025-06-19T10:32:56.287945Z",
     "iopub.status.idle": "2025-06-19T10:33:21.101148Z",
     "shell.execute_reply": "2025-06-19T10:33:21.100579Z",
     "shell.execute_reply.started": "2025-06-19T10:32:56.288489Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_exts = ('.jpg', '.jpeg', '.png')\n",
    "dataset_dir = \"/kaggle/input/comprehensive-soil-classification-datasets/CyAUG-Dataset\"\n",
    "\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "class_names = sorted([d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))])\n",
    "class_to_idx = {name: idx for idx, name in enumerate(class_names)}\n",
    "\n",
    "for class_name in class_names:\n",
    "    class_dir = os.path.join(dataset_dir, class_name)\n",
    "    for fname in os.listdir(class_dir):\n",
    "        if not fname.lower().endswith(valid_exts):\n",
    "            continue\n",
    "        fpath = os.path.join(class_dir, fname)\n",
    "\n",
    "        # Step 1: Check with PIL\n",
    "        try:\n",
    "            with Image.open(fpath) as img:\n",
    "                img.verify()\n",
    "        except:\n",
    "            print(f\"❌ Skipping unreadable with PIL: {fpath}\")\n",
    "            continue\n",
    "\n",
    "        # Step 2: Check with TensorFlow decode\n",
    "        try:\n",
    "            raw = tf.io.read_file(fpath)\n",
    "            _ = tf.image.decode_image(raw, channels=3)\n",
    "        except:\n",
    "            print(f\"❌ Skipping unreadable with tf.image: {fpath}\")\n",
    "            continue\n",
    "\n",
    "        # Passed both checks, safe to use\n",
    "        image_paths.append(fpath)\n",
    "        labels.append(class_to_idx[class_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aab12e1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7647147,
     "sourceId": 12142148,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25.374384,
   "end_time": "2025-06-19T10:52:14.122634",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-19T10:51:48.748250",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
